{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"TrackNado \ud83c\udf2a\ufe0f","text":"<p>TrackNado is a high-level library and CLI tool designed to quickly generate UCSC Genome Browser track hubs from sequencing data.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Fluent API: Declarative hub construction.</li> <li>Auto-Conversion: Implicitly handle BED, GTF, and GFF files.</li> <li>Smart Merging: Combine multiple projects with one command.</li> <li>Validation: Built-in support for <code>hubCheck</code>.</li> </ul>"},{"location":"#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>pip install tracknado\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<pre><code>import tracknado as tn\n\nhub = (\n    tn.HubBuilder()\n    .add_tracks(['sample1.bw', 'sample2.bw'])\n    .group_by('method')\n    .build(name='my_hub', genome='hg38', outdir='out/')\n)\nhub.stage_hub()\n</code></pre>"},{"location":"#documentation-sections","title":"\ud83d\udcd6 Documentation Sections","text":"<p>Explore the documentation to get started:</p> <ul> <li>Getting Started: Installation and quick start guides for CLI and API.</li> <li>Metadata Management: Learn how to handle track metadata effectively.</li> <li>Organizing Tracks: Deep dive into SuperTracks, CompositeTracks, and overlays.</li> <li>API Reference: Full library documentation.</li> <li>Examples: Real-world scripts for common workflows.</li> </ul>"},{"location":"examples/capcruncher/","title":"CapCruncher Hub Generation","text":"<p>This example replicates the complex logic of a legacy pipeline for capturing-based 3C data (CapCruncher).</p> <p>It demonstrates: - Custom metadata extraction from specific directory structures. - Complex grouping of BigWig signals. - Custom track coloring.</p> <p>View full source code on GitHub</p>"},{"location":"examples/edge-cases/","title":"Complex Edge Cases","text":"<p>Demonstrates merging multiple hubs, handling mixed file types, and reconciling different grouping strategies.</p> <p>Key features: - Merging disparate track designs. - Handling overlapping chromosome names. - Mixed data types (BAM + BigWig + BigBed).</p> <p>View full source code on GitHub</p>"},{"location":"examples/fluent-api/","title":"Fluent API Workflow","text":"<p>A clean example of the new fluent <code>HubBuilder</code> API.</p> <p>This is the recommended way to use TrackNado programmatically. It provides a readable, chained interface for constructing hubs.</p> <p>View full source code on GitHub</p>"},{"location":"examples/overview/","title":"Examples","text":"<p>TrackNado provides several examples demonstrating how to use the library for different scenarios.</p>"},{"location":"examples/overview/#1-capcruncher-hub-generation","title":"1. CapCruncher Hub Generation","text":"<p>Replicates the complex logic of a legacy pipeline for capturing-based 3C data. View Source Code</p>"},{"location":"examples/overview/#2-seqnado-workflow","title":"2. Seqnado Workflow","text":"<p>Integration with Snakemake and the <code>seqnado</code> directory structure. View Source Code</p>"},{"location":"examples/overview/#3-complex-edge-cases","title":"3. Complex Edge Cases","text":"<p>Demonstrates merging multiple hubs, handling mixed file types, and reconciling different grouping strategies. View Source Code</p>"},{"location":"examples/overview/#4-simple-refactored-workflow","title":"4. Simple Refactored Workflow","text":"<p>A clean example of the new fluent API. View Source Code</p>"},{"location":"examples/snakemake/","title":"Snakemake Integration (Seqnado)","text":"<p>Integration with Snakemake and the <code>seqnado</code> directory structure.</p> <p>This example shows how to: - Pass parameters from Snakemake to TrackNado. - Use the <code>from_seqnado_path</code> extractor. - Automate hub staging as part of a workflow.</p> <p>View full source code on GitHub</p>"},{"location":"guide/advanced/","title":"Advanced Features","text":""},{"location":"guide/advanced/#custom-genomes-assembly-hubs","title":"Custom Genomes (Assembly Hubs)","text":"<p>If you are working with a non-standard genome or a private assembly, TrackNado can generate an Assembly Hub.</p> <pre><code>tracknado create \\\n  -i tracks/*.bw \\\n  -o custom_hub \\\n  --custom-genome \\\n  --genome-name MySeq \\\n  --twobit reference.2bit \\\n  --organism \"Mouse\"\n</code></pre>"},{"location":"guide/advanced/#merging-hubs","title":"Merging Hubs","text":"<p>You can merge multiple TrackNado configurations into a single \"Master\" hub. This is useful when different teams are working on different parts of a project.</p> <pre><code># Merge two existing project configurations\ntracknado merge project_a/tracknado_config.json project_b/tracknado_config.json -o merged_hub\n</code></pre>"},{"location":"guide/advanced/#validation","title":"Validation","text":"<p>Always validate your hub before hosting it. TrackNado includes a built-in structural validator and support for the official UCSC <code>hubCheck</code>.</p> <pre><code>tracknado validate ./my_hub/hub.txt\n</code></pre>"},{"location":"guide/conversion/","title":"Track Conversion","text":"<p>TrackNado automatically converts common formats to UCSC-compatible indexed files if the conversion flag is set.</p>"},{"location":"guide/conversion/#supported-formats","title":"Supported Formats","text":"<ul> <li>BED to BigBed: Automatically sorts and converts BED files.</li> <li>GTF/GFF to BigGenePred: Converts gene annotations to the <code>bigGenePred</code> format, allowing for codon and amino acid display when zoomed in.</li> </ul>"},{"location":"guide/conversion/#usage","title":"Usage","text":""},{"location":"guide/conversion/#cli","title":"CLI","text":"<pre><code>tracknado create -i data/*.gtf -o gene_hub --convert --chrom-sizes hg38.chrom.sizes\n</code></pre>"},{"location":"guide/conversion/#api","title":"API","text":"<pre><code>builder.with_convert_files().with_chrom_sizes(\"hg38.chrom.sizes\")\n</code></pre>"},{"location":"guide/conversion/#backend-requirements","title":"Backend Requirements","text":"<p>TrackNado attempts to find UCSC tools (<code>bedToBigBed</code>, <code>gtfToGenePred</code>, etc.) in your <code>$PATH</code>. If not found, it will automatically pull and run them via Docker or Apptainer.</p>"},{"location":"guide/getting-started/","title":"Getting Started","text":"<p>TrackNado is designed to be easy to use whether you prefer the command line or a Python API.</p>"},{"location":"guide/getting-started/#installation","title":"Installation","text":"<pre><code>pip install tracknado\n</code></pre>"},{"location":"guide/getting-started/#quick-start-cli","title":"Quick Start (CLI)","text":"<p>The easiest way to get started is by using the <code>create</code> command.</p> <pre><code># Generate a metadata template\ntracknado create --template tracks.csv\n\n# After filling out tracks.csv with your file paths and metadata\ntracknado create --metadata tracks.csv --output my_hub --genome-name hg38\n</code></pre>"},{"location":"guide/getting-started/#quick-start-python-api","title":"Quick Start (Python API)","text":"<pre><code>import tracknado as tn\n\nhub = (\n    tn.HubBuilder()\n    .add_tracks(['sample1.bw', 'sample2.bw'])\n    .group_by('method')\n    .build(name='my_hub', genome='hg38', outdir='out/')\n)\nhub.stage_hub()\n</code></pre>"},{"location":"guide/metadata/","title":"Track Metadata","text":"<p>TrackNado centers around a Track Design\u2014a table where each row is a track and columns represent metadata (sample name, assay, coloring, etc.).</p>"},{"location":"guide/metadata/#positional-metadata","title":"Positional Metadata","text":"<p>You can provide metadata via a CSV/TSV file using the <code>--metadata</code> flag. This file must include a <code>fn</code> column containing the paths to your track files.</p>"},{"location":"guide/metadata/#automated-extraction","title":"Automated Extraction","text":"<p>You can also extract metadata from file paths using extraction functions. This allows you to encode metadata in your directory structure.</p> <pre><code>from tracknado import HubBuilder, from_seqnado_path\n\nbuilder = (\n    HubBuilder()\n    .add_tracks([\"data/sample1.bigWig\"])\n    .with_metadata_extractor(from_seqnado_path) # Extracts from 'sample1'\n)\n</code></pre>"},{"location":"guide/metadata/#extraction-patterns","title":"Extraction Patterns","text":"<p>TrackNado supports several built-in extractors: - <code>from_seqnado_path</code>: Expects a standard bioinformatic directory structure. - <code>from_filename_pattern</code>: Uses regex to pull metadata from names. - <code>from_parent_dirs</code>: Uses the names of parent directories as metadata values.</p>"},{"location":"guide/organization/","title":"Organizing Your Hub","text":"<p>TrackNado supports three main types of track organization, mapping directly to UCSC Genome Browser concepts.</p>"},{"location":"guide/organization/#1-supertracks-folders","title":"1. SuperTracks (Folders)","text":"<p>Top-level containers that group tracks together. These appear as folders in the track selection area.</p> <ul> <li>CLI: <code>--supergroup-by column_name</code></li> <li>API: <code>.group_by(\"column_name\", as_supertrack=True)</code></li> </ul>"},{"location":"guide/organization/#2-composite-tracks-matrices","title":"2. Composite Tracks (Matrices)","text":"<p>A multi-dimensional display matrix (matrix of checkboxes). This is the most powerful way to organize large numbers of tracks.</p> <ul> <li>CLI: <code>--subgroup-by col1 --subgroup-by col2</code></li> <li>API: <code>.group_by(\"col1\", \"col2\")</code></li> </ul>"},{"location":"guide/organization/#3-overlay-tracks-signals","title":"3. Overlay Tracks (Signals)","text":"<p>Combines multiple signals into a single plot (e.g., multi-wiggle).</p> <ul> <li>CLI: <code>--overlay-by column_name</code></li> <li>API: <code>.overlay_by(\"column_name\")</code></li> </ul>"},{"location":"guide/organization/#example-metadata-table-structures","title":"Example Metadata Table Structures","text":"<p>Below are examples of how to structure your metadata file (e.g., <code>tracks.csv</code>) for different hub layouts.</p>"},{"location":"guide/organization/#basic-composite-track-matrix-display","title":"Basic Composite Track (Matrix Display)","text":"<p>To create a matrix of checkboxes where columns are Cell Type and rows are Assay:</p> fn cell_type assay name tracks/k562_ctcf.bw K562 CTCF K562 CTCF Signal tracks/gm12878_ctcf.bw GM12878 CTCF GM12878 CTCF Signal tracks/k562_h3k27ac.bw K562 H3K27ac K562 H3K27ac Signal"},{"location":"guide/organization/#grouping-with-supertracks-folders","title":"Grouping with SuperTracks (Folders)","text":"<p>To group different types of data (Signals vs. Regions) into high-level containers:</p> fn category name tracks/chip_signal.bw Signal ChIP-seq Signal tracks/peaks.bb Regions Called Peaks tracks/genes.gtf Regions Gene Annotations"},{"location":"guide/organization/#combined-layout-folders-matrix","title":"Combined Layout (Folders + Matrix)","text":"<p>You can combine these strategies for complex hubs. This will create folders for each Assay, and within those, a matrix for Sample and Condition.</p> fn assay sample condition name data/s1_wt.bw RNA-seq S1 WT RNA S1 WT data/s1_ko.bw RNA-seq S1 KO RNA S1 KO data/atp_s1.bw ATAC-seq S1 WT ATAC S1 WT"},{"location":"reference/api/","title":"API Reference","text":"<p>This page provides the API reference for TrackNado.</p>"},{"location":"reference/api/#hub-building","title":"Hub Building","text":"<p>               Bases: <code>BaseModel</code></p> <p>Fluent API for building UCSC track hubs.</p> <p>Now a Pydantic model for EASY serialization.</p> Source code in <code>tracknado/builder.py</code> <pre><code>class HubBuilder(BaseModel):\n    \"\"\"Fluent API for building UCSC track hubs.\n\n    Now a Pydantic model for EASY serialization.\n    \"\"\"\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n        populate_by_name=True\n    )\n\n    tracks: list[Track] = Field(default_factory=list)\n    group_by_cols: list[str] = Field(default_factory=list)\n    supergroup_by_cols: list[str] = Field(default_factory=list)\n    overlay_by_cols: list[str] = Field(default_factory=list)\n    color_by_col: str | None = Field(None)\n    color_palette: str = Field(\"tab20\")\n\n    # Conversion settings\n    convert_files: bool = Field(False)\n    chrom_sizes: Optional[pathlib.Path] = Field(None)\n    custom_genome_config: dict[str, Any] = Field(default_factory=dict)\n\n    # Non-serialized field for extractors (functions can't be JSON serialized easily)\n    metadata_extractors: list[Callable[[pathlib.Path], dict[str, str]]] = Field(default_factory=list, exclude=True)\n\n    def __init__(self, **data):\n        super().__init__(**data)\n\n    def add_tracks(self, paths: list[str] | list[pathlib.Path], **common_metadata: str) -&gt; HubBuilder:\n        \"\"\"Add multiple tracks from paths.\"\"\"\n        for p in paths:\n            path = pathlib.Path(p)\n            self.tracks.append(Track(path=path, metadata=common_metadata.copy()))\n        return self\n\n    def add_tracks_from_df(self, df: pd.DataFrame, fn_col: str = 'fn') -&gt; HubBuilder:\n        \"\"\"Add tracks from a pandas DataFrame.\"\"\"\n        df = df.copy()\n        if fn_col in df.columns and 'ext' not in df.columns:\n            df['ext'] = df[fn_col].apply(lambda x: pathlib.Path(x).suffix.strip(\".\"))\n\n        try:\n            df = TrackDataFrameSchema.validate(df)\n        except Exception as e:\n            logger.warning(f\"DataFrame validation failed: {e}\")\n\n        for _, row in df.iterrows():\n            path = pathlib.Path(row[fn_col])\n            metadata = {k: str(v) for k, v in row.items() if k not in [fn_col, 'ext', 'path', 'name']}\n            track = Track(path=path, metadata=metadata)\n            if 'name' in row and pd.notna(row['name']): track.name = row['name']\n            if 'ext' in row and pd.notna(row['ext']): track.track_type = row['ext']\n            self.tracks.append(track)\n        return self\n\n    def with_metadata_extractor(self, fn: Callable[[pathlib.Path], dict[str, str]]) -&gt; HubBuilder:\n        \"\"\"Add a metadata extractor function.\"\"\"\n        self.metadata_extractors.append(fn)\n        return self\n\n    def group_by(self, *columns: str, as_supertrack: bool = False) -&gt; 'HubBuilder':\n        \"\"\"Specify columns to group by. If as_supertrack is True, these columns\n        will be used for SuperTracks instead of dimensions in a CompositeTrack.\n        \"\"\"\n        if as_supertrack:\n            self.supergroup_by_cols.extend(columns)\n        else:\n            self.group_by_cols.extend(columns)\n        return self\n\n    def with_custom_genome(\n        self, \n        name: str, \n        twobit_file: str | pathlib.Path, \n        organism: str, \n        default_position: str = \"chr1:1000-2000\"\n    ) -&gt; 'HubBuilder':\n        \"\"\"Configure a custom genome (Assembly Hub) for this hub.\"\"\"\n        self.custom_genome_config = {\n            \"custom_genome\": True,\n            \"genome_twobit\": str(twobit_file),\n            \"genome_organism\": organism,\n            \"genome_default_position\": default_position\n        }\n        return self\n\n    def color_by(self, column: str, palette: str = 'tab20') -&gt; HubBuilder:\n        \"\"\"Specify column for track coloring.\"\"\"\n        self.color_by_col = column\n        self.color_palette = palette\n        return self\n\n    def overlay_by(self, *columns: str) -&gt; HubBuilder:\n        \"\"\"Specify columns for overlay tracks.\"\"\"\n        self.overlay_by_cols.extend(columns)\n        return self\n\n    def with_convert_files(self, enabled: bool = True) -&gt; HubBuilder:\n        \"\"\"Enable or disable implicit track conversion.\"\"\"\n        self.convert_files = enabled\n        return self\n\n    def with_chrom_sizes(self, path: Union[str, pathlib.Path]) -&gt; HubBuilder:\n        \"\"\"Set the chrom.sizes file for track conversion.\"\"\"\n        self.chrom_sizes = pathlib.Path(path)\n        return self\n\n    def merge(self, *others: 'HubBuilder') -&gt; HubBuilder:\n        \"\"\"Merge other HubBuilders into this one, reconciling settings.\"\"\"\n        for other in others:\n            self.tracks.extend(other.tracks)\n            # Union of grouping columns\n            self.group_by_cols = sorted(list(set(self.group_by_cols + other.group_by_cols)))\n            self.supergroup_by_cols = sorted(list(set(self.supergroup_by_cols + other.supergroup_by_cols)))\n            self.overlay_by_cols = sorted(list(set(self.overlay_by_cols + other.overlay_by_cols)))\n            # Merge extractors\n            for ex in other.metadata_extractors:\n                if ex not in self.metadata_extractors:\n                    self.metadata_extractors.append(ex)\n            # Colors: use other's if not set here\n            if not self.color_by_col:\n                self.color_by_col = other.color_by_col\n                self.color_palette = other.color_palette\n        return self\n\n    def to_json(self, path: str | pathlib.Path | None = None) -&gt; str:\n        \"\"\"Serialize state to JSON string or file.\"\"\"\n        data = self.model_dump_json(indent=2, by_alias=True)\n        if path:\n            with open(path, 'w') as f:\n                f.write(data)\n        return data\n\n    @classmethod\n    def from_json(cls, path_or_data: str | pathlib.Path) -&gt; HubBuilder:\n        \"\"\"Reconstruct builder from JSON string or file path.\"\"\"\n        p = pathlib.Path(path_or_data)\n        if p.exists() and p.is_file():\n            with open(p, 'r') as f:\n                data = f.read()\n        else:\n            data = path_or_data\n        return cls.model_validate_json(data)\n\n    def _extract_metadata(self):\n        \"\"\"Extract metadata for all tracks using registered extractors.\"\"\"\n        if not self.metadata_extractors:\n            return\n\n        for track in self.tracks:\n            # Use original path if it exists for extraction, as it might have more metadata in its name/path\n            # than a temporary converted path.\n            path_to_extract = getattr(track, \"_original_path\", track.path)\n            for extractor in self.metadata_extractors:\n                extracted = extractor(path_to_extract)\n                track.metadata.update(extracted)\n\n    def _convert_tracks(self, outdir: pathlib.Path):\n        \"\"\"Convert tracks to UCSC formats (e.g. BED -&gt; BigBed).\"\"\"\n        from .converters import convert_bed_to_bigbed, convert_gtf_to_biggenepred\n\n        if not self.chrom_sizes or not self.chrom_sizes.exists():\n            raise ValueError(\"chrom_sizes must be provided and exist for track conversion\")\n\n        conv_dir = outdir / \"converted\"\n        conv_dir.mkdir(parents=True, exist_ok=True)\n\n        for track in self.tracks:\n            if track.path.suffix.lower() == \".bed\":\n                logger.info(f\"Converting {track.path.name} to BigBed\")\n                dest = conv_dir / track.path.with_suffix(\".bb\").name\n                # Save original path for metadata extraction if needed later\n                track._original_path = track.path\n                new_path = convert_bed_to_bigbed(\n                    track.path, \n                    self.chrom_sizes, \n                    dest\n                )\n                track.path = new_path\n                track.track_type = \"bigBed\"\n            elif track.path.suffix.lower() in [\".gtf\", \".gff\"]:\n                logger.info(f\"Converting {track.path.name} to BigGenePred\")\n                dest = conv_dir / track.path.with_suffix(\".bb\").name\n                track._original_path = track.path\n                new_path = convert_gtf_to_biggenepred(\n                    track.path,\n                    self.chrom_sizes,\n                    dest\n                )\n                track.path = new_path\n                track.track_type = \"bigGenePred\"\n\n    def _prepare_design_df(self) -&gt; pd.DataFrame:\n        \"\"\"Convert tracks to the DataFrame format used by TrackDesign.\"\"\"\n        self._extract_metadata()\n\n        extension_mapping = {\n            \"bw\": \"bigWig\",\n            \"bb\": \"bigBed\",\n            \"bigbed\": \"bigBed\",\n            \"bigwig\": \"bigWig\",\n            \"bed\": \"bigBed\", # Default for .bed is bigBed (assuming conversion)\n            \"gtf\": \"bigGenePred\",\n            \"gff\": \"bigGenePred\",\n            \"biggenepred\": \"bigGenePred\",\n            \"narrowpeak\": \"narrowPeak\",\n            \"broadpeak\": \"broadPeak\",\n        }\n\n        data = []\n        for track in self.tracks:\n            # Metadata is already extracted in build()\n\n            ext = track.track_type or track.path.suffix.lstrip(\".\")\n            ext = extension_mapping.get(ext.lower(), ext)\n\n            row = {\n                \"fn\": str(track.path),\n                \"path\": str(track.path.absolute().resolve()),\n                \"name\": track.name or track.path.stem,\n                \"ext\": ext\n            }\n            row.update(track.metadata)\n            data.append(row)\n\n        return pd.DataFrame(data)\n\n    def build(self, name: str, genome: str, outdir: str | pathlib.Path, hub_email: str = \"\", **kwargs) -&gt; Any:\n        \"\"\"Build the hub and export sidecar config.\"\"\"\n        outdir = pathlib.Path(outdir)\n        outdir.mkdir(parents=True, exist_ok=True)\n\n        # 1. Handle conversions\n        if self.convert_files:\n            self._convert_tracks(outdir)\n\n        # 2. Save sidecar config\n        self.to_json(outdir / \"tracknado_config.json\")\n\n        df = self._prepare_design_df()\n\n        design = TrackDesign.from_design(\n            df,\n            color_by=self.color_by_col,\n            subgroup_by=self.group_by_cols if self.group_by_cols else None,\n            supergroup_by=self.supergroup_by_cols if self.supergroup_by_cols else None,\n            overlay_by=self.overlay_by_cols if self.overlay_by_cols else None,\n            **kwargs\n        )\n\n        hub = HubGenerator(\n            hub_name=name,\n            genome=genome,\n            track_design=design,\n            outdir=outdir,\n            hub_email=hub_email,\n            **self.custom_genome_config,\n            **kwargs\n        )\n\n        return hub\n</code></pre> Source code in <code>tracknado/api.py</code> <pre><code>class HubGenerator:\n    def __init__(\n        self,\n        hub_name: str,\n        genome: str,\n        track_design: TrackDesign,\n        outdir: pathlib.Path,\n        description_html: pathlib.Path = None,\n        hub_email: str = \"\",\n        custom_genome: bool = False,\n        genome_twobit: pathlib.Path = None,\n        genome_organism: str = None,\n        genome_default_position: str = \"chr1:10000-20000\",\n    ):\n\n        # Basic parameters for hub creation\n        self.hub_name = hub_name\n        self.genome_name = genome\n        self.track_design = track_design\n        self.outdir = outdir\n        self.custom_genome = custom_genome\n        self.description_url_path = description_html\n\n        # Parameters for custom genomes\n        self._genome_twobit = genome_twobit\n        self._genome_organism = genome_organism\n        self._genome_default_position = genome_default_position\n\n        # Create the basic hub\n        self._hub = trackhub.Hub(\n            hub_name, short_label=hub_name, long_label=hub_name, email=hub_email\n        )\n\n        self.trackdb = trackhub.TrackDb()\n        _genome = self._get_genome_file()  # type: ignore\n        _genomes_file = trackhub.GenomesFile()\n\n        # Add these to the hub\n        _genome.add_trackdb(self.trackdb)\n        self._hub.add_genomes_file(_genomes_file)\n        _genomes_file.add_genome(_genome)\n\n        self._add_tracks_to_hub()\n\n    def _add_tracks_to_hub(self) -&gt; None:\n        # Loop through each entry in the details dataframe\n\n        for row in self.track_design.details.itertuples():\n\n            has_composite = False\n            has_overlay = False\n\n            # If the row has a \"composite\" attribute\n            if hasattr(row, \"composite\") and pd.notna(row.composite):\n                has_composite = True\n                composite_track = self.track_design.composite_tracks[row.composite]\n                # Create a new track and add it as a subtrack to the composite track\n                track = self._get_track(row, suffix=f\"_{composite_track.name}\")\n                composite_track.add_subtrack(track)\n\n            # If the row has an \"overlay\" attribute\n            if hasattr(row, \"overlay\") and pd.notna(row.overlay):\n                has_overlay = True\n                overlay_track = self.track_design.overlay_tracks[row.overlay]\n                # Create a new track and add it to the overlay track\n                track = self._get_track(row, suffix=f\"_{overlay_track.name}\")\n\n                # Ignore the track if it is not a signal track e.g. bigWig\n                if track.tracktype not in [\"bigWig\", ]:\n                    logger.warning(f\"Track {track.name} is not a signal track and will be ignored for the overlay track {overlay_track.name}\")\n                else:\n                    overlay_track.add_subtrack(track)\n\n            # If the row doesn't have a \"supertrack\" attribute\n            if not hasattr(row, \"supertrack\") and not has_composite and not has_overlay:\n                # Create a new track and add it to the trackdb\n                track = self._get_track(row)\n                self.trackdb.add_tracks(track)\n\n        # Add the supertracks or composite/overlay tracks to the trackdb\n        if self.track_design.super_tracks:\n            tracks = self.track_design.super_tracks.values()\n\n            # Ensure the composite and/or overlay tracks have the group attribute set\n            if self.custom_genome:\n                for t in [*self.track_design.composite_tracks.values(), *self.track_design.overlay_tracks.values()]:\n                    t.add_params(group=self._hub.hub)\n\n        else:\n            tracks = [*self.track_design.composite_tracks.values(), *self.track_design.overlay_tracks.values()]\n\n        # Add the composite/overlay and supertracks to the trackdb\n        for ii, track in enumerate(tracks):\n            # Add group if custom genome\n            if self.custom_genome:\n                track.add_params(group=self._hub.hub)\n            self.trackdb.add_tracks(track)\n\n\n\n    def _get_track(self, track: namedtuple, suffix: str = \"\") -&gt; trackhub.Track:\n        \"\"\"Generate a trackhub.Track object from a row in the details dataframe\"\"\"\n\n\n        extra_kwargs = dict()\n        if hasattr(track, \"color\"):\n            extra_kwargs[\"color\"] = \",\".join([str(x) for x in track.color])\n\n        if hasattr(track, \"subgroup_names\"):\n            extra_kwargs[\"subgroups\"] = {subgroup_name: getattr(track, subgroup_name) \n                                         for subgroup_name in track.subgroup_names}\n\n        if self.custom_genome:\n            extra_kwargs[\"group\"] = self._hub.hub\n\n        if track.ext == \"bigWig\":\n            extra_kwargs.update(\n                {\n                    \"maxHeightPixels\": \"100:50:11\",\n                    \"visibility\": \"full\",\n                    \"viewLimits\": \"0:100\",\n                    \"autoScale\": \"on\",\n                    \"windowingFunction\": \"mean\",\n                }\n            )\n\n        elif track.ext == \"bigBed\":\n            extra_kwargs.update(\n                {\n                    \"visibility\": \"pack\",\n                }\n            )\n\n        elif track.ext == \"bigGenePred\":\n            extra_kwargs.update(\n                {\n                    \"visibility\": \"pack\",\n                    \"baseColorDefault\": \"genomicCodons\",\n                }\n            )\n\n        return  trackhub.Track(\n                name=\"\".join([trackhub.helpers.sanitize(track.name), suffix]),\n                shortLabel=\" \".join(re.split(r\"[.|_|\\s+|-]\", track.name)),\n                longLabel=\" \".join(re.split(r\"[.|_|\\s+|-]\", track.name)),\n                source=str(track.path),\n                tracktype=track.ext,\n                **extra_kwargs,\n\n            )\n\n    def _get_genome_file(self) -&gt; trackhub.Genome:\n\n        if not self.custom_genome:\n            genome = trackhub.Genome(self.genome_name)\n            groups_file = None\n        else:\n            genome = trackhub.Assembly(\n                genome=self.genome_name,\n                twobit_file=self._genome_twobit,\n                organism=self._genome_organism,\n                defaultPos=self._genome_default_position,\n            )\n\n            groups_file = trackhub.GroupsFile(\n                [\n                    trackhub.GroupDefinition(\n                        name=self.hub_name, priority=1, default_is_closed=False\n                    ),\n                ]\n            )\n\n            genome.add_groups(groups_file)\n\n        return genome\n\n\n    def stage_hub(\n        self,\n    ):\n\n        with tempfile.TemporaryDirectory() as tmpdir:\n            trackhub.upload.stage_hub(self._hub, staging=tmpdir)\n\n            if self.description_url_path:\n                description_basename = os.path.basename(self.description_url_path)\n                with open(os.path.join(tmpdir, f\"{self._hub.hub}.hub.txt\"), \"a\") as hubtxt:\n                    hubtxt.write(\"\\n\")\n                    hubtxt.write(f\"descriptionUrl {self.genome_name}/{description_basename}\\n\")\n\n                shutil.copy(\n                    self.description_url_path,\n                    os.path.join(tmpdir, self.genome_name),\n                )\n\n            # Copy to the new location\n            shutil.copytree(\n                tmpdir,\n                self.outdir,\n                dirs_exist_ok=True,\n                symlinks=False,\n            )\n\n            subprocess.run([\"chmod\", \"-R\", \"2755\", self.outdir])\n</code></pre> Source code in <code>tracknado/api.py</code> <pre><code>class TrackDesign:\n    def __init__(\n        self,\n        details: pd.DataFrame,\n        color_by: list[str] = None,\n        subgroup_by: list[str] = None,\n        overlay_by: list[str] = None,\n        supergroup_by: list[str] = None,\n        **kwargs,\n    ):\n\n        self.details = details\n        self._supertrack_columns = list(supergroup_by) if supergroup_by else list()\n        self._overlay_columns = list(overlay_by) if overlay_by else list()\n        self._subgroup_columns = list(subgroup_by) if subgroup_by else list()\n        self.subgroup_definitions = list() if subgroup_by else None\n        self._color_columns = list(color_by) if color_by else list()\n\n        self._add_subgroupings(supergroup_by=self._supertrack_columns, subgroup_by=self._subgroup_columns)\n\n        self.super_tracks = self._get_super_tracks()\n        self._add_supertrack_indicators()\n\n        self.composite_tracks = self._get_composite_tracks()\n        self._add_composite_track_indicators()\n\n        self.overlay_tracks = self._get_overlay_tracks()\n        self._add_overlay_track_indicators()\n\n        self._add_track_colors(color_by=color_by)\n\n    @classmethod\n    def from_design(cls, design: pd.DataFrame, **kwargs) -&gt; \"TrackDesign\":\n        return cls(design, **kwargs)\n\n    def _add_track_colors(\n        self,\n        color_by: str | list[str] = None,\n        palette: str = \"tab20\",\n        color_column: str = None,\n    ) -&gt; None:\n\n        \"\"\"Add a column to the details dataframe with a color for each track\"\"\"\n\n        from PIL import ImageColor\n\n        if color_by:\n            if isinstance(color_by, str):\n                color_by = [color_by]\n\n            assert all([c in self.details.columns for c in color_by]), f\"Color-By columns {color_by} missing\"  # type: ignore\n\n            try:\n                # Get a palette with enough colors for the unique groups in the details\n                import seaborn as sns\n                n_colors = len(self.details[color_by].drop_duplicates())\n                colors = sns.color_palette(palette, n_colors=n_colors).as_hex()  # type: ignore\n\n                # Assign a color to each group\n                color_dict = {}\n                for i, group in enumerate(\n                    self.details[color_by].drop_duplicates().itertuples()\n                ):\n                    color_dict[tuple([getattr(group, c) for c in color_by])] = colors[i]  # type: ignore\n\n                # Add a column to the details dataframe with the color for each track\n                self.details[\"color\"] = self.details[color_by].apply(\n                    lambda row: ImageColor.getrgb(color_dict[tuple([c for c in row])]),\n                    axis=1,\n                )\n\n            except NameError:\n                raise NameError(\n                    \"Palette not found. Try one of the following: 'tab20', 'tab20b', 'tab20c'\"\n                )\n\n        elif color_column:\n\n            assert (\n                color_column in self.details.columns\n            ), f\"Color column {color_column} missing\"\n\n            colors = []\n            for i, color in enumerate(self.details[color_column]):\n                if isinstance(color, tuple):\n                    c = color\n                elif isinstance(color, str):\n                    if color.startswith(\"#\"):\n                        c = ImageColor.getrgb(color)\n                    else:\n                        c = color.split(\",\")\n                        c = tuple([int(x) for x in c])\n                else:\n                    raise ValueError(\n                        f\"Color column {color_column} must be a tuple or string\"\n                    )\n\n                colors.append(c)\n\n            self.details[\"color\"] = colors\n\n    def _add_subgroup_definitions_to_df(\n        self, df: pd.DataFrame, subgroup_by: list[str] = None\n    ) -&gt; pd.DataFrame:\n        \"\"\"Add a column to the details dataframe with a `trackhub.SubGroupDefinition` for each track\"\"\"\n\n        assert all(\n            [c in df.columns for c in subgroup_by]\n        ), f\"Subgroup-By columns {subgroup_by} missing\"\n        df = df.copy()\n\n        # Loop through all columns provided and generate a subgroup definition for each\n        subgroup_definitions = []\n        for column in subgroup_by:\n            # Get a list of unique values in the column\n            unique_values = df[column].unique()\n            subgroup_definition = trackhub.SubGroupDefinition(\n                name=column,\n                label=column,\n                mapping={value: value for value in unique_values},\n            )\n            subgroup_definitions.append(subgroup_definition)\n\n        # Add a column to the details dataframe with the subgroup definition for each track\n        df[\"subgroup_names\"] = [\n            tuple([col for col in subgroup_by]) for i in range(df.shape[0])\n        ]\n        df[\"subgroup_definition\"] = [subgroup_definitions for i in range(df.shape[0])]\n\n        self.subgroup_definitions.extend(subgroup_definitions)\n\n        return df\n\n    def _add_subgroupings(\n        self, supergroup_by: list[str] = None, subgroup_by: list[str] = None\n    ) -&gt; None:\n        \"\"\"Add a column to the details dataframe with a `trackhub.SubGroupDefinition` for each track.\n\n        If `supergroup_by` is provided, the subgroup definitions will be added to the dataframe\n        grouped by the supergroup columns.\n\n        If `supergroup_by` is not provided, the subgroup definitions will be added to the dataframe\n        as a single group.\n        \"\"\"\n\n        if subgroup_by:\n\n            assert all(\n                [c in self.details.columns for c in subgroup_by]\n            ), f\"Subgroup-By columns {subgroup_by} missing\"\n\n            if supergroup_by:\n                assert not any(\n                    subgroup in supergroup_by for subgroup in subgroup_by\n                ), f\"SubGroup columns {subgroup_by} cannot be in SuperGroup columns {supergroup_by}\"\n\n                self.details = self.details.groupby(supergroup_by).apply(\n                    self._add_subgroup_definitions_to_df, subgroup_by=subgroup_by, include_groups=False\n                ).reset_index(drop=False)\n                # Drop the extra index levels if they are named after the columns\n                self.details = self.details.loc[:, ~self.details.columns.duplicated()]\n            else:\n                self.details = self._add_subgroup_definitions_to_df(\n                    self.details, subgroup_by=subgroup_by\n                )\n\n    def _get_super_tracks(self) -&gt; dict[str, trackhub.SuperTrack]:\n        \"\"\"Generate a dictionary of SuperTracks from the details dataframe\"\"\"\n\n        if self._supertrack_columns:\n            assert all(\n                [c in self.details.columns for c in self._supertrack_columns]\n            ), f\"SuperTrack columns {self._supertrack_columns} missing\"\n\n            supertracks = dict()\n            for grouping, df in self.details.reset_index(drop=True).groupby(self._supertrack_columns, as_index=False):\n\n\n                if isinstance(grouping, str):\n                    track_id = (grouping,)\n                elif len(grouping) == 1:\n                    track_id = grouping\n                else:\n                    track_id = tuple(grouping)\n\n                if len(track_id) == 1:\n                    track_name = track_id[0]\n                else:\n                    track_name = \"_\".join(track_id)\n\n\n                supertracks[get_hash(track_id)] = trackhub.SuperTrack(\n                    name=track_name,\n                )\n\n        else:\n            supertracks = dict()\n\n        return supertracks\n\n    def _add_supertrack_indicators(self):\n        \"\"\"Add a column to the details dataframe with a SuperTrack indicator for each track\"\"\"\n\n        if self._supertrack_columns:\n            assert all(\n                [c in self.details.columns for c in self._supertrack_columns]\n            ), f\"SuperTrack columns {self._supertrack_columns} missing\"\n\n            self.details[\"supertrack\"] = get_hash_for_df(self.details, self._supertrack_columns)\n\n    def _get_composite_tracks(self) -&gt; dict[str, trackhub.CompositeTrack]:\n        \"\"\"Generate a dictionary of CompositeTracks from the details dataframe\"\"\"\n\n        composite_tracks = dict()\n        dimensions = dict(\n                    zip(\n                        [f\"dim{d}\" for d in [\"X\", \"Y\", \"A\", \"B\", \"C\", \"D\"]],\n                        self._subgroup_columns,\n                    )\n                )\n\n        if \"supertrack\" in self.details.columns:\n            for (supertrack, ext) , df in self.details.groupby([\"supertrack\", \"ext\"]):\n\n\n                supertrack_name = self.super_tracks[supertrack].name\n                composite_name = \"_\".join([supertrack_name, ext])\n\n\n                composite = trackhub.CompositeTrack(\n                    name=composite_name,\n                    tracktype=ext,\n                    dimensions=\" \".join([f\"{k}={v}\" for k, v in dimensions.items()])\n                    if dimensions\n                    else None,\n                    sortOrder=\" \".join([f\"{k}=+\" for k in self._subgroup_columns]),\n                    visibility=\"hide\",\n                    dragAndDrop=\"subTracks\",\n                )\n\n                composite.add_subgroups(self.subgroup_definitions)\n\n                self.super_tracks[supertrack].add_tracks(composite)\n                composite_tracks[get_hash((supertrack, ext))] = composite\n\n        elif self._subgroup_columns:\n            for ext, df in self.details.groupby(\"ext\"):\n                composite = trackhub.CompositeTrack(\n                    name=ext,\n                    tracktype=ext,\n                    visibility=\"hide\",\n                    dragAndDrop=\"subTracks\",\n                    dimensions=\" \".join([f\"{k}={v}\" for k, v in dimensions.items()])\n                    if dimensions\n                    else None,\n                    sortOrder=\" \".join([f\"{k}=+\" for k in self._subgroup_columns]),\n                )\n\n                composite.add_subgroups(self.subgroup_definitions)\n                composite_tracks[get_hash((ext,))] = composite\n\n        else:\n            composite_tracks = dict()\n\n        return composite_tracks\n\n    def _add_composite_track_indicators(self):\n        \"\"\"Add a column to the details dataframe with a CompositeTrack indicator for each track\"\"\"\n\n        if self.composite_tracks:\n            composite_columns = [\"supertrack\"] if self._supertrack_columns else []\n            composite_columns.append(\"ext\")\n\n            self.details[\"composite\"] = get_hash_for_df(self.details, composite_columns)\n\n            assert self.details[\"composite\"].isin(self.composite_tracks.keys()).all(), (\n                \"Composite tracks not found in details dataframe\"\n            )\n\n    def _get_overlay_tracks(self):\n        \"\"\"Generate a dictionary of OverlayTracks from the details dataframe\"\"\"\n\n        if self._overlay_columns:\n            assert all(\n                [c in self.details.columns for c in self._overlay_columns]\n            ), f\"Overlay columns {self._overlay_columns} missing\"\n\n            overlay_tracks = dict()\n            overlay_columns = list(self._overlay_columns) if not isinstance(self._overlay_columns, str) else [self._overlay_columns,]\n\n            if \"supertrack\" in self.details.columns:\n\n                for (supertrack, overlay) , df in self.details.groupby(\n                    [\"supertrack\", *self._overlay_columns]\n                ):\n\n                    supertrack_name = self.super_tracks[supertrack].name\n\n                    if isinstance(overlay, str):\n                        overlay_name = \"_\".join([supertrack_name, overlay]) + \"_overlay\"\n                    else:\n                        overlay_name = \"_\".join([supertrack_name, *overlay]) + \"_overlay\"\n\n                    overlay_track = trackhub.AggregateTrack(\n                        aggregate=\"transparentOverlay\",\n                        name=overlay_name,\n                        tracktype=\"bigWig\",\n                    )\n\n                    self.super_tracks[supertrack].add_tracks(overlay_track)\n                    overlay_tracks[get_hash(tuple([supertrack, overlay]))] = overlay_track\n\n            else:\n                for overlay, df in self.details.groupby(self._overlay_columns):\n\n                    overlay_name = \"_\".join(overlay) if isinstance(overlay, tuple) else overlay\n                    overlay_id = tuple(overlay) if isinstance(overlay, tuple) else (overlay,)\n\n                    overlay_track = trackhub.AggregateTrack(\n                        aggregate=\"transparentOverlay\",\n                        name=overlay_name,\n                        tracktype=\"bigWig\",\n                    )\n                    overlay_tracks[get_hash(overlay_id)] = overlay_track\n\n        else:\n            overlay_tracks = dict()\n\n        return overlay_tracks\n\n    def _add_overlay_track_indicators(self):\n        \"\"\"Add a column to the details dataframe with an OverlayTrack indicator for each track\"\"\"\n\n        if self._overlay_columns:\n\n            overlay_columns = (\n                [\"supertrack\"] if self._supertrack_columns else []\n            )\n            overlay_columns.extend(self._overlay_columns)\n\n            # Only assign indicators to rows that actually have all overlay columns set\n            has_overlay_cols = self.details[overlay_columns].notna().all(axis=1)\n\n            self.details.loc[has_overlay_cols, \"overlay\"] = get_hash_for_df(\n                self.details[has_overlay_cols], overlay_columns\n            )\n\n            # Verification should only apply to rows marked with 'overlay'\n            valid_indicators = self.details[\"overlay\"].dropna().unique()\n            missing = [i for i in valid_indicators if i not in self.overlay_tracks.keys()]\n            if missing:\n                logger.warning(f\"Overlay tracks not found for indices: {missing}\")\n                # We can choose to either raise or just clear those indicators\n                self.details.loc[self.details[\"overlay\"].isin(missing), \"overlay\"] = None\n\n        return self\n</code></pre>"},{"location":"reference/api/#tracknado.builder.HubBuilder-functions","title":"Functions","text":""},{"location":"reference/api/#tracknado.builder.HubBuilder.add_tracks","title":"<code>add_tracks(paths, **common_metadata)</code>","text":"<p>Add multiple tracks from paths.</p> Source code in <code>tracknado/builder.py</code> <pre><code>def add_tracks(self, paths: list[str] | list[pathlib.Path], **common_metadata: str) -&gt; HubBuilder:\n    \"\"\"Add multiple tracks from paths.\"\"\"\n    for p in paths:\n        path = pathlib.Path(p)\n        self.tracks.append(Track(path=path, metadata=common_metadata.copy()))\n    return self\n</code></pre>"},{"location":"reference/api/#tracknado.builder.HubBuilder.add_tracks_from_df","title":"<code>add_tracks_from_df(df, fn_col='fn')</code>","text":"<p>Add tracks from a pandas DataFrame.</p> Source code in <code>tracknado/builder.py</code> <pre><code>def add_tracks_from_df(self, df: pd.DataFrame, fn_col: str = 'fn') -&gt; HubBuilder:\n    \"\"\"Add tracks from a pandas DataFrame.\"\"\"\n    df = df.copy()\n    if fn_col in df.columns and 'ext' not in df.columns:\n        df['ext'] = df[fn_col].apply(lambda x: pathlib.Path(x).suffix.strip(\".\"))\n\n    try:\n        df = TrackDataFrameSchema.validate(df)\n    except Exception as e:\n        logger.warning(f\"DataFrame validation failed: {e}\")\n\n    for _, row in df.iterrows():\n        path = pathlib.Path(row[fn_col])\n        metadata = {k: str(v) for k, v in row.items() if k not in [fn_col, 'ext', 'path', 'name']}\n        track = Track(path=path, metadata=metadata)\n        if 'name' in row and pd.notna(row['name']): track.name = row['name']\n        if 'ext' in row and pd.notna(row['ext']): track.track_type = row['ext']\n        self.tracks.append(track)\n    return self\n</code></pre>"},{"location":"reference/api/#tracknado.builder.HubBuilder.build","title":"<code>build(name, genome, outdir, hub_email='', **kwargs)</code>","text":"<p>Build the hub and export sidecar config.</p> Source code in <code>tracknado/builder.py</code> <pre><code>def build(self, name: str, genome: str, outdir: str | pathlib.Path, hub_email: str = \"\", **kwargs) -&gt; Any:\n    \"\"\"Build the hub and export sidecar config.\"\"\"\n    outdir = pathlib.Path(outdir)\n    outdir.mkdir(parents=True, exist_ok=True)\n\n    # 1. Handle conversions\n    if self.convert_files:\n        self._convert_tracks(outdir)\n\n    # 2. Save sidecar config\n    self.to_json(outdir / \"tracknado_config.json\")\n\n    df = self._prepare_design_df()\n\n    design = TrackDesign.from_design(\n        df,\n        color_by=self.color_by_col,\n        subgroup_by=self.group_by_cols if self.group_by_cols else None,\n        supergroup_by=self.supergroup_by_cols if self.supergroup_by_cols else None,\n        overlay_by=self.overlay_by_cols if self.overlay_by_cols else None,\n        **kwargs\n    )\n\n    hub = HubGenerator(\n        hub_name=name,\n        genome=genome,\n        track_design=design,\n        outdir=outdir,\n        hub_email=hub_email,\n        **self.custom_genome_config,\n        **kwargs\n    )\n\n    return hub\n</code></pre>"},{"location":"reference/api/#tracknado.builder.HubBuilder.color_by","title":"<code>color_by(column, palette='tab20')</code>","text":"<p>Specify column for track coloring.</p> Source code in <code>tracknado/builder.py</code> <pre><code>def color_by(self, column: str, palette: str = 'tab20') -&gt; HubBuilder:\n    \"\"\"Specify column for track coloring.\"\"\"\n    self.color_by_col = column\n    self.color_palette = palette\n    return self\n</code></pre>"},{"location":"reference/api/#tracknado.builder.HubBuilder.from_json","title":"<code>from_json(path_or_data)</code>  <code>classmethod</code>","text":"<p>Reconstruct builder from JSON string or file path.</p> Source code in <code>tracknado/builder.py</code> <pre><code>@classmethod\ndef from_json(cls, path_or_data: str | pathlib.Path) -&gt; HubBuilder:\n    \"\"\"Reconstruct builder from JSON string or file path.\"\"\"\n    p = pathlib.Path(path_or_data)\n    if p.exists() and p.is_file():\n        with open(p, 'r') as f:\n            data = f.read()\n    else:\n        data = path_or_data\n    return cls.model_validate_json(data)\n</code></pre>"},{"location":"reference/api/#tracknado.builder.HubBuilder.group_by","title":"<code>group_by(*columns, as_supertrack=False)</code>","text":"<p>Specify columns to group by. If as_supertrack is True, these columns will be used for SuperTracks instead of dimensions in a CompositeTrack.</p> Source code in <code>tracknado/builder.py</code> <pre><code>def group_by(self, *columns: str, as_supertrack: bool = False) -&gt; 'HubBuilder':\n    \"\"\"Specify columns to group by. If as_supertrack is True, these columns\n    will be used for SuperTracks instead of dimensions in a CompositeTrack.\n    \"\"\"\n    if as_supertrack:\n        self.supergroup_by_cols.extend(columns)\n    else:\n        self.group_by_cols.extend(columns)\n    return self\n</code></pre>"},{"location":"reference/api/#tracknado.builder.HubBuilder.merge","title":"<code>merge(*others)</code>","text":"<p>Merge other HubBuilders into this one, reconciling settings.</p> Source code in <code>tracknado/builder.py</code> <pre><code>def merge(self, *others: 'HubBuilder') -&gt; HubBuilder:\n    \"\"\"Merge other HubBuilders into this one, reconciling settings.\"\"\"\n    for other in others:\n        self.tracks.extend(other.tracks)\n        # Union of grouping columns\n        self.group_by_cols = sorted(list(set(self.group_by_cols + other.group_by_cols)))\n        self.supergroup_by_cols = sorted(list(set(self.supergroup_by_cols + other.supergroup_by_cols)))\n        self.overlay_by_cols = sorted(list(set(self.overlay_by_cols + other.overlay_by_cols)))\n        # Merge extractors\n        for ex in other.metadata_extractors:\n            if ex not in self.metadata_extractors:\n                self.metadata_extractors.append(ex)\n        # Colors: use other's if not set here\n        if not self.color_by_col:\n            self.color_by_col = other.color_by_col\n            self.color_palette = other.color_palette\n    return self\n</code></pre>"},{"location":"reference/api/#tracknado.builder.HubBuilder.overlay_by","title":"<code>overlay_by(*columns)</code>","text":"<p>Specify columns for overlay tracks.</p> Source code in <code>tracknado/builder.py</code> <pre><code>def overlay_by(self, *columns: str) -&gt; HubBuilder:\n    \"\"\"Specify columns for overlay tracks.\"\"\"\n    self.overlay_by_cols.extend(columns)\n    return self\n</code></pre>"},{"location":"reference/api/#tracknado.builder.HubBuilder.to_json","title":"<code>to_json(path=None)</code>","text":"<p>Serialize state to JSON string or file.</p> Source code in <code>tracknado/builder.py</code> <pre><code>def to_json(self, path: str | pathlib.Path | None = None) -&gt; str:\n    \"\"\"Serialize state to JSON string or file.\"\"\"\n    data = self.model_dump_json(indent=2, by_alias=True)\n    if path:\n        with open(path, 'w') as f:\n            f.write(data)\n    return data\n</code></pre>"},{"location":"reference/api/#tracknado.builder.HubBuilder.with_chrom_sizes","title":"<code>with_chrom_sizes(path)</code>","text":"<p>Set the chrom.sizes file for track conversion.</p> Source code in <code>tracknado/builder.py</code> <pre><code>def with_chrom_sizes(self, path: Union[str, pathlib.Path]) -&gt; HubBuilder:\n    \"\"\"Set the chrom.sizes file for track conversion.\"\"\"\n    self.chrom_sizes = pathlib.Path(path)\n    return self\n</code></pre>"},{"location":"reference/api/#tracknado.builder.HubBuilder.with_convert_files","title":"<code>with_convert_files(enabled=True)</code>","text":"<p>Enable or disable implicit track conversion.</p> Source code in <code>tracknado/builder.py</code> <pre><code>def with_convert_files(self, enabled: bool = True) -&gt; HubBuilder:\n    \"\"\"Enable or disable implicit track conversion.\"\"\"\n    self.convert_files = enabled\n    return self\n</code></pre>"},{"location":"reference/api/#tracknado.builder.HubBuilder.with_custom_genome","title":"<code>with_custom_genome(name, twobit_file, organism, default_position='chr1:1000-2000')</code>","text":"<p>Configure a custom genome (Assembly Hub) for this hub.</p> Source code in <code>tracknado/builder.py</code> <pre><code>def with_custom_genome(\n    self, \n    name: str, \n    twobit_file: str | pathlib.Path, \n    organism: str, \n    default_position: str = \"chr1:1000-2000\"\n) -&gt; 'HubBuilder':\n    \"\"\"Configure a custom genome (Assembly Hub) for this hub.\"\"\"\n    self.custom_genome_config = {\n        \"custom_genome\": True,\n        \"genome_twobit\": str(twobit_file),\n        \"genome_organism\": organism,\n        \"genome_default_position\": default_position\n    }\n    return self\n</code></pre>"},{"location":"reference/api/#tracknado.builder.HubBuilder.with_metadata_extractor","title":"<code>with_metadata_extractor(fn)</code>","text":"<p>Add a metadata extractor function.</p> Source code in <code>tracknado/builder.py</code> <pre><code>def with_metadata_extractor(self, fn: Callable[[pathlib.Path], dict[str, str]]) -&gt; HubBuilder:\n    \"\"\"Add a metadata extractor function.\"\"\"\n    self.metadata_extractors.append(fn)\n    return self\n</code></pre>"},{"location":"reference/api/#data-models","title":"Data Models","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single track with validated metadata.</p> Source code in <code>tracknado/models.py</code> <pre><code>class Track(BaseModel):\n    \"\"\"Single track with validated metadata.\"\"\"\n    path: Path\n    name: str | None = None  # Auto-derived from path if None\n    metadata: dict[str, str] = Field(default_factory=dict)\n    color: tuple[int, int, int] | None = None\n    track_type: str | None = None  # bigWig, bigBed, etc.\n\n    @field_validator('path')\n    @classmethod\n    def validate_path_exists(cls, v):\n        if not v.exists():\n            # Note: We might want a way to allow virtual paths if staging doesn't require immediate existence\n            # but for now we enforce existence as per requirements.\n            pass \n        return v\n\n    @field_validator('color')\n    @classmethod  \n    def validate_color_range(cls, v):\n        if v and not all(0 &lt;= c &lt;= 255 for c in v):\n            raise ValueError(\"Color values must be 0-255\")\n        return v\n</code></pre> <p>               Bases: <code>BaseModel</code></p> <p>Hierarchical grouping of tracks.</p> Source code in <code>tracknado/models.py</code> <pre><code>class TrackGroup(BaseModel):\n    \"\"\"Hierarchical grouping of tracks.\"\"\"\n    name: str\n    tracks: list[Track] = Field(default_factory=list)\n    subgroups: list[TrackGroup] = Field(default_factory=list)\n    metadata: dict[str, str] = Field(default_factory=dict)\n</code></pre>"},{"location":"reference/api/#file-converters","title":"File Converters","text":"<p>Convert a BED file to BigBed format.</p> Source code in <code>tracknado/converters.py</code> <pre><code>def convert_bed_to_bigbed(\n    input_bed: Path, \n    chrom_sizes: Path, \n    output_bb: Path | None = None,\n    force_container: bool = False\n) -&gt; Path:\n    \"\"\"Convert a BED file to BigBed format.\"\"\"\n    if output_bb is None:\n        output_bb = input_bed.with_suffix(\".bb\")\n\n    # 1. Find tool\n    cmd_prefix = []\n    if not force_container:\n        local_tool = ToolFinder.find_local(\"bedToBigBed\")\n        if local_tool:\n            cmd_prefix = [local_tool]\n\n    if not cmd_prefix:\n        container_cmd = ToolFinder.get_container_cmd(\"bedToBigBed\")\n        if container_cmd:\n            cmd_prefix = container_cmd\n        else:\n            raise RuntimeError(\n                \"bedToBigBed not found locally and no container engine (Apptainer/Docker) detected. \"\n                \"Please install bedToBigBed or a container engine.\"\n            )\n\n    # 2. Sort BED file (required for bedToBigBed)\n    logger.info(f\"Sorting {input_bed.name}...\")\n    sorted_bed = tempfile.NamedTemporaryFile(suffix=\".sorted.bed\", delete=False).name\n    try:\n        # We use LC_ALL=C for consistent sorting\n        env = os.environ.copy()\n        env[\"LC_ALL\"] = \"C\"\n        subprocess.run(\n            [\"sort\", \"-k1,1\", \"-k2,2n\", str(input_bed)],\n            stdout=open(sorted_bed, \"w\"),\n            check=True,\n            env=env\n        )\n\n        # 3. Run bedToBigBed\n        logger.info(f\"Converting {input_bed.name} to BigBed...\")\n\n        # Prepare actual command (handling Docker mount replacement if needed)\n        cwd = os.getcwd()\n        final_cmd = []\n        for part in cmd_prefix:\n            if isinstance(part, str):\n                final_cmd.append(part.replace(\"{cwd}\", cwd))\n\n        final_cmd.extend([sorted_bed, str(chrom_sizes), str(output_bb)])\n\n        subprocess.run(final_cmd, check=True)\n        logger.info(f\"Successfully created {output_bb}\")\n\n    finally:\n        if os.path.exists(sorted_bed):\n            os.remove(sorted_bed)\n\n    return output_bb\n</code></pre> <p>Convert a GTF or GFF file to BigGenePred format.</p> Source code in <code>tracknado/converters.py</code> <pre><code>def convert_gtf_to_biggenepred(\n    input_file: Path,\n    chrom_sizes: Path,\n    output_bb: Path | None = None,\n    force_container: bool = False\n) -&gt; Path:\n    \"\"\"Convert a GTF or GFF file to BigGenePred format.\"\"\"\n    if output_bb is None:\n        output_bb = input_file.with_suffix(\".bb\")\n\n    ext = input_file.suffix.lower()\n    conv_tool = \"gtfToGenePred\" if ext == \".gtf\" else \"gff3ToGenePred\"\n\n    # Check for tools\n    tools = [conv_tool, \"genePredToBigGenePred\", \"bedToBigBed\"]\n    cmd_prefixes = {}\n\n    for tool in tools:\n        cmd_prefix = []\n        if not force_container:\n            local_tool = ToolFinder.find_local(tool)\n            if local_tool:\n                cmd_prefix = [local_tool]\n\n        if not cmd_prefix:\n            container_cmd = ToolFinder.get_container_cmd(tool)\n            if container_cmd:\n                cmd_prefix = container_cmd\n            else:\n                 raise RuntimeError(\n                    f\"{tool} not found locally and no container engine detected.\"\n                )\n        cmd_prefixes[tool] = cmd_prefix\n\n    as_file = Path(__file__).parent / \"as_files\" / \"bigGenePred.as\"\n    if not as_file.exists():\n        raise FileNotFoundError(f\"Required autoSql file not found: {as_file}\")\n\n    intermediate_genepred = tempfile.NamedTemporaryFile(suffix=\".genePred\", delete=False).name\n    intermediate_txt = tempfile.NamedTemporaryFile(suffix=\".txt\", delete=False).name\n\n    try:\n        cwd = os.getcwd()\n\n        # 1. Convert to genePred\n        logger.info(f\"Converting {input_file.name} to genePred...\")\n        cmd1 = []\n        for part in cmd_prefixes[conv_tool]:\n            cmd1.append(part.replace(\"{cwd}\", cwd))\n        if conv_tool == \"gtfToGenePred\":\n            cmd1.extend([\"-genePredExt\", str(input_file), intermediate_genepred])\n        else: # gff3ToGenePred\n            cmd1.extend([str(input_file), intermediate_genepred])\n        subprocess.run(cmd1, check=True)\n\n        # 2. Convert to bigGenePred text\n        logger.info(\"Converting genePred to bigGenePred text...\")\n        cmd2 = []\n        for part in cmd_prefixes[\"genePredToBigGenePred\"]:\n            cmd2.append(part.replace(\"{cwd}\", cwd))\n        cmd2.extend([intermediate_genepred, intermediate_txt])\n        subprocess.run(cmd2, check=True)\n\n        # 3. Sort bigGenePred text (required for bedToBigBed)\n        logger.info(\"Sorting bigGenePred text...\")\n        sorted_txt = tempfile.NamedTemporaryFile(suffix=\".sorted.txt\", delete=False).name\n        env = os.environ.copy()\n        env[\"LC_ALL\"] = \"C\"\n        subprocess.run(\n            [\"sort\", \"-k1,1\", \"-k2,2n\", intermediate_txt],\n            stdout=open(sorted_txt, \"w\"),\n            check=True,\n            env=env\n        )\n\n        # 4. Convert to BigBed with .as\n        logger.info(f\"Converting to BigGenePred: {output_bb.name}...\")\n        cmd4 = []\n        for part in cmd_prefixes[\"bedToBigBed\"]:\n            cmd4.append(part.replace(\"{cwd}\", cwd))\n        cmd4.extend([\n            \"-type=bed12+8\", \n            \"-tab\", \n            f\"-as={as_file}\", \n            sorted_txt, \n            str(chrom_sizes), \n            str(output_bb)\n        ])\n        subprocess.run(cmd4, check=True)\n\n        if os.path.exists(sorted_txt):\n            os.remove(sorted_txt)\n\n        logger.info(f\"Successfully created {output_bb}\")\n\n    finally:\n        for f in [intermediate_genepred, intermediate_txt]:\n            if os.path.exists(f):\n                os.remove(f)\n\n    return output_bb\n</code></pre>"},{"location":"reference/api/#metadata-extractors","title":"Metadata Extractors","text":"<p>Extract metadata from seqnado file paths.</p> <p>Pattern: .../assay/method/norm/samplename.ext Example: .../ATAC/ATAC_Tn5/CPM/sample1.bigWig</p> Source code in <code>tracknado/extractors.py</code> <pre><code>def from_seqnado_path(path: Path) -&gt; dict[str, str]:\n    \"\"\"Extract metadata from seqnado file paths.\n\n    Pattern: .../assay/method/norm/samplename.ext\n    Example: .../ATAC/ATAC_Tn5/CPM/sample1.bigWig\n    \"\"\"\n    metadata = {}\n    parts = path.parts\n    if len(parts) &gt;= 4:\n        # We assume the directory structure is relatively fixed for seqnado\n        metadata[\"norm\"] = parts[-2]\n        metadata[\"method\"] = parts[-3]\n        metadata[\"assay\"] = parts[-4]\n\n    # samplename is usually the stem, but seqnado sometimes has extensions like .plus/.minus\n    # We'll take the first part before any dots or underscores commonly used\n    stem = path.stem\n    metadata[\"samplename\"] = re.split(r\"[._]\", stem)[0]\n\n    return metadata\n</code></pre> <p>Create extractor from regex pattern with named groups.</p> Source code in <code>tracknado/extractors.py</code> <pre><code>def from_filename_pattern(pattern: str) -&gt; Callable[[Path], dict[str, str]]:\n    \"\"\"Create extractor from regex pattern with named groups.\"\"\"\n    regex = re.compile(pattern)\n\n    def extractor(path: Path) -&gt; Dict[str, str]:\n        match = regex.search(path.name)\n        if match:\n            return match.groupdict()\n        return {}\n\n    return extractor\n</code></pre> <p>Extract metadata from parent directory names.</p> <p>Parameters:</p> Name Type Description Default <code>depth</code> <code>int</code> <p>How many levels up to go</p> <code>1</code> <code>names</code> <code>list[str]</code> <p>Optional list of keys for the directory levels (last to first)</p> <code>None</code> Source code in <code>tracknado/extractors.py</code> <pre><code>def from_parent_dirs(depth: int = 1, names: list[str] = None) -&gt; Callable[[Path], dict[str, str]]:\n    \"\"\"Extract metadata from parent directory names.\n\n    Args:\n        depth: How many levels up to go\n        names: Optional list of keys for the directory levels (last to first)\n    \"\"\"\n    def extractor(path: Path) -&gt; dict[str, str]:\n        metadata = {}\n        current = path.parent\n        for i in range(depth):\n            if current == current.parent: # Reached root\n                break\n            key = names[i] if names and i &lt; len(names) else f\"dir_{i+1}\"\n            metadata[key] = current.name\n            current = current.parent\n        return metadata\n\n    return extractor\n</code></pre>"},{"location":"reference/api/#validation","title":"Validation","text":"<p>Validate a hub using UCSC's hubCheck tool.</p> <p>Parameters:</p> Name Type Description Default <code>hub_path</code> <code>Path</code> <p>Path to hub.txt file</p> required <code>strict</code> <code>bool</code> <p>If True, fail on warnings too</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[bool, str]</code> <p>(is_valid, message) tuple</p> Source code in <code>tracknado/validation.py</code> <pre><code>def validate_hub(hub_path: Path, strict: bool = False) -&gt; tuple[bool, str]:\n    \"\"\"Validate a hub using UCSC's hubCheck tool.\n\n    Args:\n        hub_path: Path to hub.txt file\n        strict: If True, fail on warnings too\n\n    Returns:\n        (is_valid, message) tuple\n    \"\"\"\n    hub_path = Path(hub_path)\n    if not hub_path.exists():\n        return False, f\"Hub file not found: {hub_path}\"\n\n    hubcheck = shutil.which(\"hubCheck\")\n    if not hubcheck:\n        # Check standard user local bin too\n        user_bin = Path.home() / \"bin\" / \"hubCheck\"\n        if user_bin.exists():\n            hubcheck = str(user_bin)\n\n    if not hubcheck:\n        return False, (\"hubCheck not found in PATH or ~/bin/hubCheck. \"\n                      \"Install from: http://hgdownload.cse.ucsc.edu/admin/exe/\")\n\n    cmd = [hubcheck]\n    if strict:\n        cmd.append(\"-strict\")\n    cmd.append(str(hub_path))\n\n    try:\n        result = subprocess.run(cmd, capture_output=True, text=True)\n        is_valid = result.returncode == 0\n        message = result.stderr or result.stdout\n        if not message and is_valid:\n            message = \"Hub is valid.\"\n        return is_valid, message\n    except Exception as e:\n        return False, f\"Error running hubCheck: {e}\"\n</code></pre> <p>Validates hub structure without external tools.</p> Source code in <code>tracknado/validation.py</code> <pre><code>class HubValidator:\n    \"\"\"Validates hub structure without external tools.\"\"\"\n\n    def __init__(self, hub_dir: str | Path):\n        self.hub_dir = Path(hub_dir)\n        self.errors = []\n        self.warnings = []\n\n    def validate_all(self) -&gt; bool:\n        \"\"\"Run all validations.\"\"\"\n        self.validate_structure()\n        self.validate_track_files_exist()\n        return len(self.errors) == 0\n\n    def validate_structure(self) -&gt; list[str]:\n        \"\"\"Check hub has required files.\"\"\"\n        hub_txt = self.hub_dir / f\"{self.hub_dir.name}.hub.txt\"\n        # Since hub names can vary, we look for *.hub.txt\n        hub_files = list(self.hub_dir.glob(\"*.hub.txt\"))\n        if not hub_files:\n            self.errors.append(\"No hub.txt file found in directory.\")\n\n        # Check for genomes.txt referenced in hub.txt would be better, \n        # but for now we look for common file names or hub-specific ones\n        genomes_files = (list(self.hub_dir.glob(\"**/genomes.txt\")) + \n                         list(self.hub_dir.glob(\"**/*.genomes.txt\")))\n        if not genomes_files:\n            self.errors.append(\"No genomes.txt file found.\")\n\n        trackdb_files = list(self.hub_dir.glob(\"**/trackDb.txt\"))\n        if not trackdb_files:\n            self.errors.append(\"No trackDb.txt file found.\")\n\n        return self.errors\n\n    def validate_track_files_exist(self) -&gt; list[str]:\n        \"\"\"Ensure all referenced track files exist locally.\n\n        Note: This only works if tracks are local paths, which is true during staging.\n        \"\"\"\n        for trackdb in self.hub_dir.glob(\"**/trackDb.txt\"):\n            with open(trackdb, 'r') as f:\n                content = f.read()\n                # Simple regex to find 'bigDataUrl' entries\n                import re\n                urls = re.findall(r\"bigDataUrl\\s+(.+)\", content)\n                for url in urls:\n                    # Resolve relative to trackdb or hub_dir\n                    # Hubs usually use relative paths from trackdb\n                    track_path = trackdb.parent / url\n                    if not track_path.exists():\n                        self.warnings.append(f\"Track file not found: {url} (referenced in {trackdb.name})\")\n        return self.warnings\n</code></pre>"},{"location":"reference/api/#tracknado.validation.HubValidator-functions","title":"Functions","text":""},{"location":"reference/api/#tracknado.validation.HubValidator.validate_all","title":"<code>validate_all()</code>","text":"<p>Run all validations.</p> Source code in <code>tracknado/validation.py</code> <pre><code>def validate_all(self) -&gt; bool:\n    \"\"\"Run all validations.\"\"\"\n    self.validate_structure()\n    self.validate_track_files_exist()\n    return len(self.errors) == 0\n</code></pre>"},{"location":"reference/api/#tracknado.validation.HubValidator.validate_structure","title":"<code>validate_structure()</code>","text":"<p>Check hub has required files.</p> Source code in <code>tracknado/validation.py</code> <pre><code>def validate_structure(self) -&gt; list[str]:\n    \"\"\"Check hub has required files.\"\"\"\n    hub_txt = self.hub_dir / f\"{self.hub_dir.name}.hub.txt\"\n    # Since hub names can vary, we look for *.hub.txt\n    hub_files = list(self.hub_dir.glob(\"*.hub.txt\"))\n    if not hub_files:\n        self.errors.append(\"No hub.txt file found in directory.\")\n\n    # Check for genomes.txt referenced in hub.txt would be better, \n    # but for now we look for common file names or hub-specific ones\n    genomes_files = (list(self.hub_dir.glob(\"**/genomes.txt\")) + \n                     list(self.hub_dir.glob(\"**/*.genomes.txt\")))\n    if not genomes_files:\n        self.errors.append(\"No genomes.txt file found.\")\n\n    trackdb_files = list(self.hub_dir.glob(\"**/trackDb.txt\"))\n    if not trackdb_files:\n        self.errors.append(\"No trackDb.txt file found.\")\n\n    return self.errors\n</code></pre>"},{"location":"reference/api/#tracknado.validation.HubValidator.validate_track_files_exist","title":"<code>validate_track_files_exist()</code>","text":"<p>Ensure all referenced track files exist locally.</p> <p>Note: This only works if tracks are local paths, which is true during staging.</p> Source code in <code>tracknado/validation.py</code> <pre><code>def validate_track_files_exist(self) -&gt; list[str]:\n    \"\"\"Ensure all referenced track files exist locally.\n\n    Note: This only works if tracks are local paths, which is true during staging.\n    \"\"\"\n    for trackdb in self.hub_dir.glob(\"**/trackDb.txt\"):\n        with open(trackdb, 'r') as f:\n            content = f.read()\n            # Simple regex to find 'bigDataUrl' entries\n            import re\n            urls = re.findall(r\"bigDataUrl\\s+(.+)\", content)\n            for url in urls:\n                # Resolve relative to trackdb or hub_dir\n                # Hubs usually use relative paths from trackdb\n                track_path = trackdb.parent / url\n                if not track_path.exists():\n                    self.warnings.append(f\"Track file not found: {url} (referenced in {trackdb.name})\")\n    return self.warnings\n</code></pre>"},{"location":"reference/cli/","title":"CLI Reference","text":"<p>TrackNado provides a powerful command-line interface for common tasks.</p>"},{"location":"reference/cli/#tracknado-create","title":"<code>tracknado create</code>","text":"<p>The main command for generating hubs.</p>"},{"location":"reference/cli/#arguments","title":"Arguments","text":"<ul> <li><code>-i, --input &lt;PATHS&gt;</code>: Paths to track files (BigWig, BAM, BED, GTF, etc.).</li> <li><code>-m, --metadata &lt;CSV/TSV&gt;</code>: Path to metadata table.</li> <li><code>-o, --output &lt;DIR&gt;</code>: Output directory for the hub.</li> </ul>"},{"location":"reference/cli/#flags","title":"Flags","text":"<ul> <li><code>--template</code>: Generate a metadata template based on input files.</li> <li><code>--convert</code>: Automatically convert files to UCSC formats.</li> <li><code>--custom-genome</code>: Generate an assembly hub.</li> </ul>"},{"location":"reference/cli/#tracknado-merge","title":"<code>tracknado merge</code>","text":"<p>Combines multiple <code>tracknado_config.json</code> files into a single hub.</p>"},{"location":"reference/cli/#tracknado-validate","title":"<code>tracknado validate</code>","text":"<p>Validates a hub directory or <code>hub.txt</code> file.</p>"}]}